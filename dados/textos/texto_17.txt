Título: Sem título

PEQUENOS MODELOS DE LINGUAGEMRicardo MurerJulho 2024INTRODUÇÃOA IA generativa, nos deu os grandes modelos de linguagem ou Large Language Models (LLMs), tais como o GPT-4 da OpenAI, Llama 3 da Meta, Google Gemini e Claude 3 da Anthropic entre outros, os quais tem demonstrado capacidades impressionantes. Imaginar as redes neurais destes LLMs com bilhões ou trilhões de parâmetros[1] é algo que foge de nossa compreensão e até mesmo os desenvolvedores que os criaram, os quais enfrentam desafios em termos de sua explicabilidade e transparência. A busca pela superinteligência tem impulsionado a OpenAI, a Meta e a Google a desenvolverem LLMs com um número cada vez maior de parâmetros. Para eles, quanto mais neurônios artificiais, maior a capacidade do modelo. No entanto, esse avanço tem um preço significativo: um consumo massivo de recursos computacionais e de energia. Mas há uma luz (e espero que seja de LED) no fim do túnel, com a chegada dos pequenos modelos de linguagem ou Small Language Models (SLMs), com menos parâmetros, foco em economia de recursos computacionais e energia. Os SLMs estão se mostrando uma alternativa viável, especialmente para pequenas e médias empresas e aplicações em domínios de conhecimento específicos. Neste artigo, vou explorar essa novidade emergente, seus benefícios, limitações e aplicações.QUANTO PEQUENO?Não há um consenso estabelecido sobre a quantidade exata de parâmetros que define um SLM. Alguns consideram que SLMs devem possuir parâmetros na ordem de milhões, como o BERT-Tiny (4,4 milhões), DistilBERT (66 milhões), TinyBERT (14,5 milhões) e MobileBERT (25 milhões) por exemplo. Outros autores argumentam que SLMs podem ter alguns bilhões de parâmetros, como o Google Gemma (1,8 bilhões), Microsoft Phi-3 (até 14 bilhões) e Mistral 7B (7 bilhões). Mas podemos assumir que o que geralmente caracteriza um SLM é possuir significativamente menos parâmetros que os LLMs, que podem ter dezenas a centenas de bilhões de parâmetros ou mais.TREINAMENTO E