Título: Redes neurais convolucionais

Uma breve história sobre as arquiteturas“Deep learning” é um dos assuntos mais populares da atualidade quando se fala em inteligência artificial. É um tópico que tem gerado bastante curiosidade, tanto para os profissionais da tecnologia e pesquisadores, quanto para pessoas externas.Atualmente, existe uma grande diversidade de aplicações que utilizam deep learning nos mais diversos cenários, inclusive aqui no Itaú. Uma particularidade, geralmente conhecida por quem trabalha na área, é que o deep learning se estende tanto aos dados estruturados quanto os não estruturados.Tradicionalmente, um modelo deep consiste em uma MultiLayer Perceptron (MLP) com diversas camadas. Mas, não para por aí: existem também as redes neurais recorrentes e as redes neurais convolucionais (Convolutional Neural Networks — CNNs), que são capazes de lidar com dados não-estruturados.Como cientista do time de Visão Computacional do Itaú, venho contar, brevemente, sobre a evolução de algumas das arquiteturas de CNNs às quais estudei durante a minha pesquisa de doutorado.ArquiteturasA primeira CNN, proposta em 1998 por LeCun, é denominada LeNet [1]. Ela foi desenvolvida com o intuito de auxiliar no reconhecimento de caracteres da base MNIST e gerou resultados com alta acurácia. Uma das particularidades mais importantes das CNNs é que os pesos a serem aprendidos nas camadas convolucionais compõem o filtro que realizará a operação de convolução. Dessa maneira, além de manter a relação espacial das imagens, são extraídas características relevantes durante a aprendizagem. Adicionalmente, o uso de camadas convolucionais reduz significativamente a quantidade de parâmetros quando comparada à MLP e facilita a convergência da rede.A arquitetura e conceitos introduzidos pela LeNet ainda são muito importantes, pois serviram como base para a criação das arquiteturas mais atuais. Em suma, a rede é formada por uma sequência de camadas, sendo que cada uma delas possui uma função específica na propagação do sinal